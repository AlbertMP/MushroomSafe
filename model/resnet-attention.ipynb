{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 4 metrics during training progress\n",
    "def plot_training_progress(results_path):\n",
    "    data = torch.load(results_path)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    plt.plot(data[\"train_loss\"], 'r', label = 'train_loss')\n",
    "    plt.plot(data[\"val_loss\"], 'm', label = 'val_loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    ax2=ax1.twinx()\n",
    "    plt.plot(data[\"train_accuracy\"], 'g', label = 'train_accuracy')\n",
    "    plt.plot(data[\"val_accuracy\"], 'b', label = 'val_accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2]*x.shape[3]).permute(0,2,1)\n",
    "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2]*x.shape[3])\n",
    "        attention = self.softmax(torch.bmm(proj_query, proj_key))\n",
    "        proj_value = self.value_conv(x).view(x.shape[0], -1, x.shape[2]*x.shape[3])\n",
    "        out = torch.bmm(proj_value, attention.permute(0,2,1)).view(x.shape)\n",
    "        out = self.gamma*out + x\n",
    "        return out, attention\n",
    "\n",
    "class AttentionResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AttentionResNet, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        self.self_attention = SelfAttention(self.resnet.fc.in_features)\n",
    "        self.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x, attention_map = self.self_attention(x)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, attention_map\n",
    "\n",
    "def visualize_attention(model, input_image):\n",
    "    model.eval()\n",
    "    _, attention_map = model(input_image)\n",
    "    attention_map = attention_map.detach().cpu().numpy()\n",
    "\n",
    "    # Normalize attention map for visualization\n",
    "    max_value = attention_map.max()\n",
    "    min_value = attention_map.min()\n",
    "    attention_map = (attention_map - min_value) / (max_value - min_value)\n",
    "\n",
    "    return attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a model and a fake input image\n",
    "model = AttentionResNet(1000) # change to the number of your classes\n",
    "input_image = torch.rand((1, 3, 224, 224))\n",
    "\n",
    "# Get the attention map of the model\n",
    "attention_map = visualize_attention(model, input_image)\n",
    "\n",
    "# Reshape attention map for visualization\n",
    "attention_map = attention_map.reshape((attention_map.shape[1], int(attention_map.shape[2]**0.5), int(attention_map.shape[2]**0.5)))\n",
    "\n",
    "# Plot the attention map\n",
    "plt.imshow(attention_map[0,:,:], cmap='hot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# time recording start\n",
    "start_time = time.time()\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "dataset = ImageFolder(root=\"./MO_106/\", transform=transform)\n",
    "\n",
    "# 数据集分割为训练集、验证集和测试集\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.00005\n",
    "\n",
    "# 分别创建训练集、验证集和测试集的 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 定义模型\n",
    "model = AttentionResNet(num_classes=106)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise ndarray to store the loss and accuracy in each epoch (on the training data)\n",
    "train_loss = np.zeros(num_epochs)\n",
    "train_accuracy = np.zeros(num_epochs)\n",
    "val_loss = np.zeros(num_epochs)\n",
    "val_accuracy = np.zeros(num_epochs)\n",
    "\n",
    "# 训练模型\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # if (i+1) % 100 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/10], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "\n",
    "    # 在每个 epoch 后使用验证集评估模型\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    correct_predictions_val = 0\n",
    "    total_predictions_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_epoch += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions_val += (predicted == labels).sum().item()\n",
    "            total_predictions_val += labels.size(0)\n",
    "    # Calculate average epoch loss and accuracy\n",
    "    train_loss[epoch] = running_loss / len(train_dataset)\n",
    "    train_accuracy[epoch] = correct_predictions / total_predictions\n",
    "    val_loss[epoch] = val_loss_epoch / len(val_dataset)\n",
    "    val_accuracy[epoch] = correct_predictions_val / total_predictions_val\n",
    "\n",
    "    # Print training and validation statistics\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss[epoch]:.4f}, Train Accuracy: {train_accuracy[epoch]:.4f}, Val Loss: {val_loss[epoch]:.4f}, Val Accuracy: {val_accuracy[epoch]:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Training took {total_time:.2f} seconds.\")\n",
    "\n",
    "model_metrics = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"train_loss\": train_loss,\n",
    "    \"train_accuracy\": train_accuracy,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"val_accuracy\": val_accuracy,\n",
    "    'total_time': total_time\n",
    "}\n",
    "\n",
    "# Save the model parameters and metrics to a file\n",
    "ResultPath = \"./results/\"\n",
    "results_path = ResultPath + f\"attention_resnet_epoch{num_epochs}_lr{lr}_bs{batch_size}.pt\"\n",
    "\n",
    "torch.save(model_metrics, results_path)\n",
    "        \n",
    "# 所有训练完成后，使用测试集进行最后的评估\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test accuracy: {correct / total * 100}%')\n",
    "\n",
    "# Post-training attention visualization\n",
    "# Use a batch from your test set as an example\n",
    "example_images, example_labels = next(iter(test_loader))\n",
    "example_images = example_images.to(device)\n",
    "\n",
    "# Get the attention map of the model\n",
    "attention_map = visualize_attention(model, example_images)\n",
    "\n",
    "# Reshape attention map for visualization\n",
    "attention_map = attention_map.reshape((attention_map.shape[0], attention_map.shape[1], int(attention_map.shape[2]**0.5), int(attention_map.shape[2]**0.5)))\n",
    "\n",
    "# Plot the attention map of the first image in the batch\n",
    "plt.imshow(attention_map[0,0,:,:], cmap='hot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MushroomSafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
